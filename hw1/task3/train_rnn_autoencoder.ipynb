{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.2\n",
    "set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embeddings_index = {}\n",
    "with open( 'glove.6B.100d.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('nodeMeta.pkl', 'rb') as f:\n",
    "    nodeMeta = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 17500 texts.\n"
     ]
    }
   ],
   "source": [
    "corpus = []\n",
    "for i in nodeMeta:\n",
    "    corpus.append(nodeMeta[i][\"abstract\"])\n",
    "print('Found %s texts.' % len(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['perform',\n",
       " 'sewing',\n",
       " 'two',\n",
       " 'dual',\n",
       " 'ramond',\n",
       " 'reggeon',\n",
       " 'vertices',\n",
       " 'derive',\n",
       " 'algorithm',\n",
       " 'means']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[3][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19801 words\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "t = Tokenizer()\n",
    "t.fit_on_texts(corpus)\n",
    "print (len(t.word_counts), 'words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab2idx = dict((i, t.word_index[i]) for i in t.word_index)\n",
    "idx2vocab = dict((t.word_index[i], i) for i in t.word_index)\n",
    "idx2vocab[0] = \"<pad>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60.5432 55.0 2 237\n"
     ]
    }
   ],
   "source": [
    "# sequence length analysis\n",
    "seq_length = [len(i) for i in corpus]\n",
    "print(np.mean(seq_length), np.median(seq_length), np.min(seq_length), np.max(seq_length))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "data = []\n",
    "for i in corpus:\n",
    "    seq = [t.word_index.get(j, 0) for j in i]\n",
    "    pad_seq = pad_sequences([seq], maxlen=35, dtype='int32', padding='post', truncating='post', value=0.0)\n",
    "    data.append(pad_seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6517 vocab has no embedding 19801\n"
     ]
    }
   ],
   "source": [
    "cnt = 0\n",
    "vocab_size = len(t.word_index)\n",
    "embedding_matrix = np.zeros((vocab_size+1, 100))\n",
    "for i in range(vocab_size+1):\n",
    "    if idx2vocab[i] in embeddings_index:\n",
    "        embedding_matrix[i] = embeddings_index[idx2vocab[i]]\n",
    "    else:\n",
    "        cnt += 1\n",
    "print (cnt, 'vocab has no embedding', vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "data = np.array(data).reshape(17500, 35)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generator(data):\n",
    "    \n",
    "    idx = np.arange(len(data))\n",
    "    while True:\n",
    "        np.random.shuffle(idx)\n",
    "        batches = [idx[range(batch_size*i, min(len(data), batch_size*(i+1)))] for i in range(len(data)//batch_size+1)] \n",
    "        \n",
    "        for i in batches:\n",
    "            yield data[i], to_categorical(data[i], num_classes=vocab_size+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, LSTM, RepeatVector, Embedding, Dense\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "timesteps = 35\n",
    "input_dim = 1\n",
    "latent_dim = 128\n",
    "\n",
    "embedding_layer = Embedding(len(t.word_index) + 1,\n",
    "                            100,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=35,\n",
    "                            trainable=False)\n",
    "\n",
    "inputs = Input(shape=(timesteps,))\n",
    "emb = embedding_layer(inputs)\n",
    "encoded = LSTM(latent_dim)(emb)\n",
    "\n",
    "decoded = RepeatVector(timesteps)(encoded)\n",
    "decoded = LSTM(100, return_sequences=True)(decoded)\n",
    "decoded = Dense(vocab_size+1, activation='softmax')(decoded)\n",
    "\n",
    "autoencoder = Model(inputs, decoded)\n",
    "encoder = Model(inputs, encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_3 (InputLayer)         (None, 35)                0         \n",
      "_________________________________________________________________\n",
      "embedding_3 (Embedding)      (None, 35, 100)           1980200   \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (None, 128)               117248    \n",
      "_________________________________________________________________\n",
      "repeat_vector_3 (RepeatVecto (None, 35, 128)           0         \n",
      "_________________________________________________________________\n",
      "lstm_6 (LSTM)                (None, 35, 100)           91600     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 35, 19802)         2000002   \n",
      "=================================================================\n",
      "Total params: 4,189,050\n",
      "Trainable params: 2,208,850\n",
      "Non-trainable params: 1,980,200\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "autoencoder .summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam\n",
    "opt = Adam(lr=0.01, amsgrad=True, decay=0.005)\n",
    "autoencoder.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = int(len(data)*0.95)\n",
    "train, val = data[:split], data[split:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "547/547 [==============================] - 104s 191ms/step - loss: 7.1732 - acc: 0.0635 - val_loss: 6.9110 - val_acc: 0.0729\n",
      "Epoch 2/500\n",
      "547/547 [==============================] - 103s 188ms/step - loss: 6.7896 - acc: 0.0782 - val_loss: 6.6184 - val_acc: 0.0821\n",
      "Epoch 3/500\n",
      "547/547 [==============================] - 103s 188ms/step - loss: 6.5906 - acc: 0.0862 - val_loss: 6.4658 - val_acc: 0.0900\n",
      "Epoch 4/500\n",
      "547/547 [==============================] - 103s 188ms/step - loss: 6.4533 - acc: 0.0935 - val_loss: 6.3419 - val_acc: 0.0963\n",
      "Epoch 5/500\n",
      "547/547 [==============================] - 103s 188ms/step - loss: 6.3306 - acc: 0.0997 - val_loss: 6.2279 - val_acc: 0.1029\n",
      "Epoch 6/500\n",
      "547/547 [==============================] - 103s 188ms/step - loss: 6.2091 - acc: 0.1051 - val_loss: 6.1120 - val_acc: 0.1081\n",
      "Epoch 7/500\n",
      "547/547 [==============================] - 103s 188ms/step - loss: 6.0935 - acc: 0.1102 - val_loss: 5.9853 - val_acc: 0.1129\n",
      "Epoch 8/500\n",
      "547/547 [==============================] - 103s 188ms/step - loss: 5.9898 - acc: 0.1143 - val_loss: 5.8985 - val_acc: 0.1172\n",
      "Epoch 9/500\n",
      "547/547 [==============================] - 103s 188ms/step - loss: 5.9044 - acc: 0.1186 - val_loss: 5.8061 - val_acc: 0.1219\n",
      "Epoch 10/500\n",
      "547/547 [==============================] - 103s 188ms/step - loss: 5.8300 - acc: 0.1220 - val_loss: 5.7527 - val_acc: 0.1239\n",
      "Epoch 11/500\n",
      "547/547 [==============================] - 103s 188ms/step - loss: 5.7578 - acc: 0.1251 - val_loss: 5.6790 - val_acc: 0.1274\n",
      "Epoch 12/500\n",
      "547/547 [==============================] - 103s 188ms/step - loss: 5.6916 - acc: 0.1282 - val_loss: 5.6444 - val_acc: 0.1267\n",
      "Epoch 13/500\n",
      "547/547 [==============================] - 103s 189ms/step - loss: 5.6368 - acc: 0.1306 - val_loss: 5.5930 - val_acc: 0.1297\n",
      "Epoch 14/500\n",
      "547/547 [==============================] - 103s 189ms/step - loss: 5.5909 - acc: 0.1331 - val_loss: 5.5139 - val_acc: 0.1376\n",
      "Epoch 15/500\n",
      "547/547 [==============================] - 103s 189ms/step - loss: 5.5501 - acc: 0.1356 - val_loss: 5.4856 - val_acc: 0.1390\n",
      "Epoch 16/500\n",
      "547/547 [==============================] - 104s 189ms/step - loss: 5.5128 - acc: 0.1377 - val_loss: 5.4504 - val_acc: 0.1411\n",
      "Epoch 17/500\n",
      "547/547 [==============================] - 103s 189ms/step - loss: 5.4794 - acc: 0.1397 - val_loss: 5.4155 - val_acc: 0.1426\n",
      "Epoch 18/500\n",
      "547/547 [==============================] - 103s 189ms/step - loss: 5.4506 - acc: 0.1419 - val_loss: 5.3850 - val_acc: 0.1445\n",
      "Epoch 19/500\n",
      "547/547 [==============================] - 103s 189ms/step - loss: 5.4226 - acc: 0.1434 - val_loss: 5.3505 - val_acc: 0.1472\n",
      "Epoch 20/500\n",
      "547/547 [==============================] - 103s 189ms/step - loss: 5.3845 - acc: 0.1449 - val_loss: 5.3207 - val_acc: 0.1476\n",
      "Epoch 21/500\n",
      "547/547 [==============================] - 103s 189ms/step - loss: 5.3444 - acc: 0.1466 - val_loss: 5.2891 - val_acc: 0.1469\n",
      "Epoch 22/500\n",
      "547/547 [==============================] - 103s 189ms/step - loss: 5.3107 - acc: 0.1481 - val_loss: 5.2348 - val_acc: 0.1534\n",
      "Epoch 23/500\n",
      "547/547 [==============================] - 103s 189ms/step - loss: 5.2847 - acc: 0.1498 - val_loss: 5.2288 - val_acc: 0.1532\n",
      "Epoch 24/500\n",
      "547/547 [==============================] - 103s 189ms/step - loss: 5.2638 - acc: 0.1517 - val_loss: 5.1974 - val_acc: 0.1567\n",
      "Epoch 25/500\n",
      "547/547 [==============================] - 103s 189ms/step - loss: 5.2449 - acc: 0.1529 - val_loss: 5.1712 - val_acc: 0.1571\n",
      "Epoch 26/500\n",
      "547/547 [==============================] - 103s 189ms/step - loss: 5.2282 - acc: 0.1547 - val_loss: 5.1651 - val_acc: 0.1562\n",
      "Epoch 27/500\n",
      "547/547 [==============================] - 104s 189ms/step - loss: 5.2072 - acc: 0.1561 - val_loss: 5.1336 - val_acc: 0.1604\n",
      "Epoch 28/500\n",
      "547/547 [==============================] - 104s 189ms/step - loss: 5.1887 - acc: 0.1572 - val_loss: 5.1159 - val_acc: 0.1615\n",
      "Epoch 29/500\n",
      "547/547 [==============================] - 103s 189ms/step - loss: 5.1736 - acc: 0.1587 - val_loss: 5.1319 - val_acc: 0.1587\n",
      "Epoch 30/500\n",
      "547/547 [==============================] - 103s 189ms/step - loss: 5.1582 - acc: 0.1599 - val_loss: 5.0918 - val_acc: 0.1628\n",
      "Epoch 31/500\n",
      "547/547 [==============================] - 104s 189ms/step - loss: 5.1446 - acc: 0.1608 - val_loss: 5.0818 - val_acc: 0.1646\n",
      "Epoch 32/500\n",
      "547/547 [==============================] - 104s 189ms/step - loss: 5.1322 - acc: 0.1624 - val_loss: 5.0725 - val_acc: 0.1671\n",
      "Epoch 33/500\n",
      "547/547 [==============================] - 104s 189ms/step - loss: 5.1200 - acc: 0.1635 - val_loss: 5.0784 - val_acc: 0.1651\n",
      "Epoch 34/500\n",
      "547/547 [==============================] - 103s 189ms/step - loss: 5.1100 - acc: 0.1648 - val_loss: 5.0456 - val_acc: 0.1707\n",
      "Epoch 35/500\n",
      "547/547 [==============================] - 104s 189ms/step - loss: 5.0999 - acc: 0.1654 - val_loss: 5.0528 - val_acc: 0.1685\n",
      "Epoch 36/500\n",
      "547/547 [==============================] - 104s 190ms/step - loss: 5.0899 - acc: 0.1670 - val_loss: 5.0216 - val_acc: 0.1708\n",
      "Epoch 37/500\n",
      "547/547 [==============================] - 104s 189ms/step - loss: 5.0795 - acc: 0.1681 - val_loss: 5.0196 - val_acc: 0.1739\n",
      "Epoch 38/500\n",
      "547/547 [==============================] - 104s 189ms/step - loss: 5.0724 - acc: 0.1691 - val_loss: 5.0031 - val_acc: 0.1772\n",
      "Epoch 39/500\n",
      "547/547 [==============================] - 104s 189ms/step - loss: 5.0660 - acc: 0.1701 - val_loss: 4.9989 - val_acc: 0.1742\n",
      "Epoch 40/500\n",
      "547/547 [==============================] - 104s 190ms/step - loss: 5.0533 - acc: 0.1710 - val_loss: 4.9935 - val_acc: 0.1745\n",
      "Epoch 41/500\n",
      "547/547 [==============================] - 104s 189ms/step - loss: 5.0429 - acc: 0.1724 - val_loss: 4.9845 - val_acc: 0.1788\n",
      "Epoch 42/500\n",
      "547/547 [==============================] - 104s 189ms/step - loss: 5.0366 - acc: 0.1732 - val_loss: 4.9741 - val_acc: 0.1787\n",
      "Epoch 43/500\n",
      "547/547 [==============================] - 103s 189ms/step - loss: 5.0291 - acc: 0.1741 - val_loss: 4.9618 - val_acc: 0.1821\n",
      "Epoch 44/500\n",
      "547/547 [==============================] - 103s 189ms/step - loss: 5.0240 - acc: 0.1751 - val_loss: 4.9606 - val_acc: 0.1807\n",
      "Epoch 45/500\n",
      "547/547 [==============================] - 103s 189ms/step - loss: 5.0188 - acc: 0.1761 - val_loss: 4.9506 - val_acc: 0.1828\n",
      "Epoch 46/500\n",
      "547/547 [==============================] - 103s 189ms/step - loss: 5.0126 - acc: 0.1767 - val_loss: 4.9421 - val_acc: 0.1828\n",
      "Epoch 47/500\n",
      "547/547 [==============================] - 103s 189ms/step - loss: 5.0059 - acc: 0.1775 - val_loss: 4.9438 - val_acc: 0.1819\n",
      "Epoch 48/500\n",
      "547/547 [==============================] - 103s 189ms/step - loss: 4.9975 - acc: 0.1782 - val_loss: 4.9380 - val_acc: 0.1825\n",
      "Epoch 49/500\n",
      "547/547 [==============================] - 103s 189ms/step - loss: 4.9894 - acc: 0.1793 - val_loss: 4.9219 - val_acc: 0.1846\n",
      "Epoch 50/500\n",
      "547/547 [==============================] - 104s 189ms/step - loss: 4.9820 - acc: 0.1801 - val_loss: 4.9187 - val_acc: 0.1855\n",
      "Epoch 51/500\n",
      "547/547 [==============================] - 104s 189ms/step - loss: 4.9731 - acc: 0.1807 - val_loss: 4.9036 - val_acc: 0.1864\n",
      "Epoch 52/500\n",
      "547/547 [==============================] - 103s 189ms/step - loss: 4.9671 - acc: 0.1812 - val_loss: 4.9033 - val_acc: 0.1852\n",
      "Epoch 53/500\n",
      "547/547 [==============================] - 110s 201ms/step - loss: 4.9575 - acc: 0.1826 - val_loss: 4.9222 - val_acc: 0.1838\n",
      "Epoch 54/500\n",
      "547/547 [==============================] - 110s 201ms/step - loss: 4.9520 - acc: 0.1831 - val_loss: 4.8815 - val_acc: 0.1888\n",
      "Epoch 55/500\n",
      "547/547 [==============================] - 107s 195ms/step - loss: 4.9452 - acc: 0.1838 - val_loss: 4.8856 - val_acc: 0.1872\n",
      "Epoch 56/500\n",
      "547/547 [==============================] - 104s 189ms/step - loss: 4.9366 - acc: 0.1842 - val_loss: 4.8709 - val_acc: 0.1889\n",
      "Epoch 57/500\n",
      "547/547 [==============================] - 104s 189ms/step - loss: 4.9297 - acc: 0.1848 - val_loss: 4.8635 - val_acc: 0.1907\n",
      "Epoch 58/500\n",
      "547/547 [==============================] - 104s 189ms/step - loss: 4.9208 - acc: 0.1849 - val_loss: 4.8670 - val_acc: 0.1873\n",
      "Epoch 59/500\n",
      "547/547 [==============================] - 103s 189ms/step - loss: 4.9100 - acc: 0.1859 - val_loss: 4.8354 - val_acc: 0.1922\n",
      "Epoch 60/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "547/547 [==============================] - 104s 189ms/step - loss: 4.8891 - acc: 0.1855 - val_loss: 4.8276 - val_acc: 0.1891\n",
      "Epoch 61/500\n",
      "547/547 [==============================] - 103s 188ms/step - loss: 4.8750 - acc: 0.1865 - val_loss: 4.8080 - val_acc: 0.1960\n",
      "Epoch 62/500\n",
      "547/547 [==============================] - 103s 188ms/step - loss: 4.8704 - acc: 0.1868 - val_loss: 4.8067 - val_acc: 0.1925\n",
      "Epoch 63/500\n",
      "547/547 [==============================] - 103s 189ms/step - loss: 4.8676 - acc: 0.1875 - val_loss: 4.8211 - val_acc: 0.1912\n",
      "Epoch 64/500\n",
      "547/547 [==============================] - 103s 189ms/step - loss: 4.8653 - acc: 0.1879 - val_loss: 4.7983 - val_acc: 0.1960\n",
      "Epoch 65/500\n",
      "547/547 [==============================] - 103s 189ms/step - loss: 4.8638 - acc: 0.1884 - val_loss: 4.8121 - val_acc: 0.1904\n",
      "Epoch 66/500\n",
      "547/547 [==============================] - 103s 189ms/step - loss: 4.8637 - acc: 0.1895 - val_loss: 4.8102 - val_acc: 0.1925\n",
      "Epoch 67/500\n",
      "547/547 [==============================] - 103s 189ms/step - loss: 4.8626 - acc: 0.1900 - val_loss: 4.8204 - val_acc: 0.1925\n",
      "Epoch 68/500\n",
      "547/547 [==============================] - 103s 189ms/step - loss: 4.8602 - acc: 0.1903 - val_loss: 4.7970 - val_acc: 0.1957\n",
      "Epoch 69/500\n",
      "547/547 [==============================] - 104s 190ms/step - loss: 4.8546 - acc: 0.1909 - val_loss: 4.7877 - val_acc: 0.1960\n",
      "Epoch 70/500\n",
      "547/547 [==============================] - 104s 191ms/step - loss: 4.8515 - acc: 0.1913 - val_loss: 4.8126 - val_acc: 0.1907\n",
      "Epoch 71/500\n",
      "547/547 [==============================] - 312s 571ms/step - loss: 4.8495 - acc: 0.1923 - val_loss: 4.7942 - val_acc: 0.1980\n",
      "Epoch 72/500\n",
      "547/547 [==============================] - 104s 191ms/step - loss: 4.8482 - acc: 0.1928 - val_loss: 4.7771 - val_acc: 0.2015\n",
      "Epoch 73/500\n",
      "547/547 [==============================] - 104s 191ms/step - loss: 4.8468 - acc: 0.1930 - val_loss: 4.8072 - val_acc: 0.1985\n",
      "Epoch 74/500\n",
      "547/547 [==============================] - 104s 191ms/step - loss: 4.8457 - acc: 0.1935 - val_loss: 4.7726 - val_acc: 0.2018\n",
      "Epoch 75/500\n",
      "547/547 [==============================] - 104s 191ms/step - loss: 4.8427 - acc: 0.1947 - val_loss: 4.7933 - val_acc: 0.1969\n",
      "Epoch 76/500\n",
      "547/547 [==============================] - 104s 191ms/step - loss: 4.8406 - acc: 0.1949 - val_loss: 4.7904 - val_acc: 0.1971\n",
      "Epoch 77/500\n",
      "547/547 [==============================] - 104s 191ms/step - loss: 4.8396 - acc: 0.1951 - val_loss: 4.7782 - val_acc: 0.2000\n",
      "Epoch 78/500\n",
      "547/547 [==============================] - 104s 191ms/step - loss: 4.8373 - acc: 0.1953 - val_loss: 4.7824 - val_acc: 0.1979\n",
      "Epoch 79/500\n",
      "547/547 [==============================] - 104s 191ms/step - loss: 4.8349 - acc: 0.1961 - val_loss: 4.7823 - val_acc: 0.2005\n",
      "Epoch 80/500\n",
      "547/547 [==============================] - 104s 191ms/step - loss: 4.8338 - acc: 0.1961 - val_loss: 4.7994 - val_acc: 0.1999\n",
      "Epoch 81/500\n",
      "547/547 [==============================] - 104s 191ms/step - loss: 4.8316 - acc: 0.1967 - val_loss: 4.7623 - val_acc: 0.2031\n",
      "Epoch 82/500\n",
      "547/547 [==============================] - 104s 191ms/step - loss: 4.8275 - acc: 0.1972 - val_loss: 4.7862 - val_acc: 0.1982\n",
      "Epoch 83/500\n",
      "547/547 [==============================] - 104s 191ms/step - loss: 4.8246 - acc: 0.1981 - val_loss: 4.8057 - val_acc: 0.1940\n",
      "Epoch 84/500\n",
      "547/547 [==============================] - 104s 191ms/step - loss: 4.8219 - acc: 0.1980 - val_loss: 4.7781 - val_acc: 0.2027\n",
      "Epoch 85/500\n",
      "547/547 [==============================] - 104s 191ms/step - loss: 4.8182 - acc: 0.1984 - val_loss: 4.7530 - val_acc: 0.2041\n",
      "Epoch 86/500\n",
      "547/547 [==============================] - 104s 191ms/step - loss: 4.8164 - acc: 0.1988 - val_loss: 4.7392 - val_acc: 0.2067\n",
      "Epoch 87/500\n",
      "547/547 [==============================] - 104s 191ms/step - loss: 4.8130 - acc: 0.1991 - val_loss: 4.7733 - val_acc: 0.1997\n",
      "Epoch 88/500\n",
      "547/547 [==============================] - 104s 190ms/step - loss: 4.8077 - acc: 0.1994 - val_loss: 4.7366 - val_acc: 0.2068\n",
      "Epoch 89/500\n",
      "547/547 [==============================] - 106s 194ms/step - loss: 4.8047 - acc: 0.1997 - val_loss: 4.7396 - val_acc: 0.2073\n",
      "Epoch 90/500\n",
      "547/547 [==============================] - 107s 196ms/step - loss: 4.8000 - acc: 0.2000 - val_loss: 4.7321 - val_acc: 0.2073\n",
      "Epoch 91/500\n",
      "547/547 [==============================] - 107s 196ms/step - loss: 4.7946 - acc: 0.2012 - val_loss: 4.7382 - val_acc: 0.2069\n",
      "Epoch 92/500\n",
      "547/547 [==============================] - 105s 192ms/step - loss: 4.7911 - acc: 0.2009 - val_loss: 4.7205 - val_acc: 0.2077\n",
      "Epoch 93/500\n",
      "547/547 [==============================] - 104s 190ms/step - loss: 4.7870 - acc: 0.2013 - val_loss: 4.7469 - val_acc: 0.2053\n",
      "Epoch 94/500\n",
      "547/547 [==============================] - 104s 191ms/step - loss: 4.7823 - acc: 0.2015 - val_loss: 4.7313 - val_acc: 0.2037\n",
      "Epoch 95/500\n",
      "547/547 [==============================] - 104s 191ms/step - loss: 4.7789 - acc: 0.2016 - val_loss: 4.7160 - val_acc: 0.2076\n",
      "Epoch 96/500\n",
      "547/547 [==============================] - 104s 191ms/step - loss: 4.7758 - acc: 0.2020 - val_loss: 4.7127 - val_acc: 0.2056\n",
      "Epoch 97/500\n",
      "547/547 [==============================] - 104s 190ms/step - loss: 4.7715 - acc: 0.2027 - val_loss: 4.6959 - val_acc: 0.2103\n",
      "Epoch 98/500\n",
      "547/547 [==============================] - 104s 191ms/step - loss: 4.7683 - acc: 0.2029 - val_loss: 4.6951 - val_acc: 0.2109\n",
      "Epoch 99/500\n",
      "547/547 [==============================] - 104s 191ms/step - loss: 4.7658 - acc: 0.2030 - val_loss: 4.6960 - val_acc: 0.2094\n",
      "Epoch 100/500\n",
      "547/547 [==============================] - 104s 191ms/step - loss: 4.7626 - acc: 0.2030 - val_loss: 4.6898 - val_acc: 0.2099\n",
      "Epoch 101/500\n",
      "547/547 [==============================] - 104s 191ms/step - loss: 4.7597 - acc: 0.2037 - val_loss: 4.6914 - val_acc: 0.2098\n",
      "Epoch 102/500\n",
      "547/547 [==============================] - 104s 191ms/step - loss: 4.7571 - acc: 0.2038 - val_loss: 4.6947 - val_acc: 0.2087\n",
      "Epoch 103/500\n",
      "547/547 [==============================] - 104s 191ms/step - loss: 4.7537 - acc: 0.2041 - val_loss: 4.7221 - val_acc: 0.2042\n",
      "Epoch 104/500\n",
      "547/547 [==============================] - 104s 191ms/step - loss: 4.7508 - acc: 0.2042 - val_loss: 4.6947 - val_acc: 0.2084\n",
      "Epoch 105/500\n",
      "547/547 [==============================] - 104s 190ms/step - loss: 4.7482 - acc: 0.2044 - val_loss: 4.6716 - val_acc: 0.2106\n",
      "Epoch 106/500\n",
      "547/547 [==============================] - 104s 191ms/step - loss: 4.7464 - acc: 0.2047 - val_loss: 4.6833 - val_acc: 0.2119\n",
      "Epoch 107/500\n",
      "547/547 [==============================] - 104s 191ms/step - loss: 4.7425 - acc: 0.2050 - val_loss: 4.6703 - val_acc: 0.2128\n",
      "Epoch 108/500\n",
      "547/547 [==============================] - 104s 191ms/step - loss: 4.7399 - acc: 0.2053 - val_loss: 4.6695 - val_acc: 0.2116\n",
      "Epoch 109/500\n",
      "547/547 [==============================] - 104s 191ms/step - loss: 4.7383 - acc: 0.2050 - val_loss: 4.6936 - val_acc: 0.2086\n",
      "Epoch 110/500\n",
      "547/547 [==============================] - 104s 191ms/step - loss: 4.7363 - acc: 0.2058 - val_loss: 4.6625 - val_acc: 0.2115\n",
      "Epoch 111/500\n",
      "547/547 [==============================] - 104s 191ms/step - loss: 4.7330 - acc: 0.2063 - val_loss: 4.6803 - val_acc: 0.2110\n",
      "Epoch 112/500\n",
      "547/547 [==============================] - 104s 191ms/step - loss: 4.7311 - acc: 0.2063 - val_loss: 4.6693 - val_acc: 0.2102\n",
      "Epoch 113/500\n",
      "547/547 [==============================] - 104s 191ms/step - loss: 4.7292 - acc: 0.2069 - val_loss: 4.6613 - val_acc: 0.2112\n",
      "Epoch 114/500\n",
      "547/547 [==============================] - 104s 191ms/step - loss: 4.7272 - acc: 0.2065 - val_loss: 4.6632 - val_acc: 0.2141\n",
      "Epoch 115/500\n",
      "547/547 [==============================] - 104s 191ms/step - loss: 4.7247 - acc: 0.2071 - val_loss: 4.6678 - val_acc: 0.2111\n",
      "Epoch 116/500\n",
      "547/547 [==============================] - 104s 191ms/step - loss: 4.7230 - acc: 0.2074 - val_loss: 4.6476 - val_acc: 0.2151\n",
      "Epoch 117/500\n",
      "547/547 [==============================] - 104s 191ms/step - loss: 4.7217 - acc: 0.2075 - val_loss: 4.6475 - val_acc: 0.2131\n",
      "Epoch 118/500\n",
      "547/547 [==============================] - 104s 191ms/step - loss: 4.7202 - acc: 0.2081 - val_loss: 4.6666 - val_acc: 0.2121\n",
      "Epoch 119/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "547/547 [==============================] - 104s 191ms/step - loss: 4.7173 - acc: 0.2081 - val_loss: 4.6493 - val_acc: 0.2171\n",
      "Epoch 120/500\n",
      "547/547 [==============================] - 104s 191ms/step - loss: 4.7159 - acc: 0.2084 - val_loss: 4.6758 - val_acc: 0.2097\n",
      "Epoch 121/500\n",
      "547/547 [==============================] - 104s 191ms/step - loss: 4.7140 - acc: 0.2087 - val_loss: 4.6458 - val_acc: 0.2161\n",
      "Epoch 122/500\n",
      "547/547 [==============================] - 104s 191ms/step - loss: 4.7115 - acc: 0.2089 - val_loss: 4.6533 - val_acc: 0.2125\n",
      "Epoch 123/500\n",
      "547/547 [==============================] - 104s 191ms/step - loss: 4.7097 - acc: 0.2096 - val_loss: 4.6415 - val_acc: 0.2149\n",
      "Epoch 124/500\n",
      "547/547 [==============================] - 104s 191ms/step - loss: 4.7085 - acc: 0.2094 - val_loss: 4.6410 - val_acc: 0.2173\n",
      "Epoch 125/500\n",
      "547/547 [==============================] - 104s 191ms/step - loss: 4.7071 - acc: 0.2094 - val_loss: 4.6521 - val_acc: 0.2148\n",
      "Epoch 126/500\n",
      "547/547 [==============================] - 104s 191ms/step - loss: 4.7061 - acc: 0.2098 - val_loss: 4.6541 - val_acc: 0.2132\n",
      "Epoch 127/500\n",
      "547/547 [==============================] - 104s 191ms/step - loss: 4.7051 - acc: 0.2098 - val_loss: 4.6232 - val_acc: 0.2192\n",
      "Epoch 128/500\n",
      "547/547 [==============================] - 104s 191ms/step - loss: 4.7022 - acc: 0.2103 - val_loss: 4.6398 - val_acc: 0.2115\n",
      "Epoch 129/500\n",
      "547/547 [==============================] - 104s 190ms/step - loss: 4.7010 - acc: 0.2100 - val_loss: 4.6611 - val_acc: 0.2117\n",
      "Epoch 130/500\n",
      "547/547 [==============================] - 104s 191ms/step - loss: 4.6995 - acc: 0.2106 - val_loss: 4.6316 - val_acc: 0.2174\n",
      "Epoch 131/500\n",
      "547/547 [==============================] - 104s 191ms/step - loss: 4.6984 - acc: 0.2108 - val_loss: 4.6312 - val_acc: 0.2174\n",
      "Epoch 132/500\n",
      "547/547 [==============================] - 104s 190ms/step - loss: 4.6966 - acc: 0.2108 - val_loss: 4.6398 - val_acc: 0.2154\n",
      "Epoch 133/500\n",
      "547/547 [==============================] - 104s 191ms/step - loss: 4.6947 - acc: 0.2110 - val_loss: 4.6213 - val_acc: 0.2181\n",
      "Epoch 134/500\n",
      "547/547 [==============================] - 104s 190ms/step - loss: 4.6937 - acc: 0.2112 - val_loss: 4.6861 - val_acc: 0.2062\n",
      "Epoch 135/500\n",
      "547/547 [==============================] - 104s 190ms/step - loss: 4.6933 - acc: 0.2115 - val_loss: 4.6182 - val_acc: 0.2197\n",
      "Epoch 136/500\n",
      "547/547 [==============================] - 135s 247ms/step - loss: 4.6900 - acc: 0.2114 - val_loss: 4.6172 - val_acc: 0.2190\n",
      "Epoch 137/500\n",
      "547/547 [==============================] - 110s 202ms/step - loss: 4.6893 - acc: 0.2117 - val_loss: 4.6186 - val_acc: 0.2185\n",
      "Epoch 138/500\n",
      "547/547 [==============================] - 111s 202ms/step - loss: 4.6883 - acc: 0.2118 - val_loss: 4.6180 - val_acc: 0.2178\n",
      "Epoch 139/500\n",
      "547/547 [==============================] - 111s 203ms/step - loss: 4.6858 - acc: 0.2123 - val_loss: 4.6336 - val_acc: 0.2143\n",
      "Epoch 140/500\n",
      "547/547 [==============================] - 111s 203ms/step - loss: 4.6842 - acc: 0.2123 - val_loss: 4.6277 - val_acc: 0.2154\n",
      "Epoch 141/500\n",
      "547/547 [==============================] - 111s 203ms/step - loss: 4.6840 - acc: 0.2121 - val_loss: 4.6084 - val_acc: 0.2196\n",
      "Epoch 142/500\n",
      "547/547 [==============================] - 108s 198ms/step - loss: 4.6822 - acc: 0.2130 - val_loss: 4.6082 - val_acc: 0.2197\n",
      "Epoch 143/500\n",
      "547/547 [==============================] - 104s 190ms/step - loss: 4.6805 - acc: 0.2125 - val_loss: 4.6093 - val_acc: 0.2190\n",
      "Epoch 144/500\n",
      "547/547 [==============================] - 104s 191ms/step - loss: 4.6783 - acc: 0.2134 - val_loss: 4.6212 - val_acc: 0.2191\n",
      "Epoch 145/500\n",
      "547/547 [==============================] - 104s 191ms/step - loss: 4.6779 - acc: 0.2137 - val_loss: 4.6380 - val_acc: 0.2138\n",
      "Epoch 146/500\n",
      "547/547 [==============================] - 104s 191ms/step - loss: 4.6756 - acc: 0.2136 - val_loss: 4.6012 - val_acc: 0.2213\n",
      "Epoch 147/500\n",
      "547/547 [==============================] - 104s 191ms/step - loss: 4.6753 - acc: 0.2137 - val_loss: 4.6173 - val_acc: 0.2166\n",
      "Epoch 148/500\n",
      "547/547 [==============================] - 104s 191ms/step - loss: 4.6740 - acc: 0.2139 - val_loss: 4.6005 - val_acc: 0.2227\n",
      "Epoch 149/500\n",
      "547/547 [==============================] - 104s 191ms/step - loss: 4.6720 - acc: 0.2141 - val_loss: 4.6023 - val_acc: 0.2204\n",
      "Epoch 150/500\n",
      "547/547 [==============================] - 104s 191ms/step - loss: 4.6704 - acc: 0.2149 - val_loss: 4.5937 - val_acc: 0.2239\n",
      "Epoch 151/500\n",
      "547/547 [==============================] - 104s 191ms/step - loss: 4.6692 - acc: 0.2139 - val_loss: 4.6018 - val_acc: 0.2207\n",
      "Epoch 152/500\n",
      "547/547 [==============================] - 104s 191ms/step - loss: 4.6682 - acc: 0.2142 - val_loss: 4.5938 - val_acc: 0.2229\n",
      "Epoch 153/500\n",
      "547/547 [==============================] - 104s 190ms/step - loss: 4.6659 - acc: 0.2148 - val_loss: 4.6029 - val_acc: 0.2213\n",
      "Epoch 154/500\n",
      "547/547 [==============================] - 104s 190ms/step - loss: 4.6657 - acc: 0.2145 - val_loss: 4.6015 - val_acc: 0.2212\n",
      "Epoch 155/500\n",
      "547/547 [==============================] - 104s 190ms/step - loss: 4.6637 - acc: 0.2149 - val_loss: 4.5889 - val_acc: 0.2195\n",
      "Epoch 156/500\n",
      "547/547 [==============================] - 104s 191ms/step - loss: 4.6640 - acc: 0.2150 - val_loss: 4.5953 - val_acc: 0.2207\n",
      "Epoch 157/500\n",
      "547/547 [==============================] - 104s 191ms/step - loss: 4.6617 - acc: 0.2153 - val_loss: 4.5817 - val_acc: 0.2253\n",
      "Epoch 158/500\n",
      "547/547 [==============================] - 104s 190ms/step - loss: 4.6599 - acc: 0.2154 - val_loss: 4.6064 - val_acc: 0.2181\n",
      "Epoch 159/500\n",
      "547/547 [==============================] - 104s 191ms/step - loss: 4.6590 - acc: 0.2156 - val_loss: 4.5825 - val_acc: 0.2229\n",
      "Epoch 160/500\n",
      "547/547 [==============================] - 104s 191ms/step - loss: 4.6571 - acc: 0.2160 - val_loss: 4.5738 - val_acc: 0.2238\n",
      "Epoch 161/500\n",
      "547/547 [==============================] - 104s 191ms/step - loss: 4.6565 - acc: 0.2160 - val_loss: 4.5929 - val_acc: 0.2216\n",
      "Epoch 162/500\n",
      "547/547 [==============================] - 104s 191ms/step - loss: 4.6550 - acc: 0.2158 - val_loss: 4.5782 - val_acc: 0.2252\n",
      "Epoch 163/500\n",
      "547/547 [==============================] - 104s 190ms/step - loss: 4.6523 - acc: 0.2169 - val_loss: 4.6153 - val_acc: 0.2158\n",
      "Epoch 164/500\n",
      "547/547 [==============================] - 104s 191ms/step - loss: 4.6508 - acc: 0.2162 - val_loss: 4.5939 - val_acc: 0.2213\n",
      "Epoch 165/500\n",
      "547/547 [==============================] - 104s 191ms/step - loss: 4.6515 - acc: 0.2166 - val_loss: 4.5747 - val_acc: 0.2244\n",
      "Epoch 166/500\n",
      "547/547 [==============================] - 104s 190ms/step - loss: 4.6494 - acc: 0.2170 - val_loss: 4.5860 - val_acc: 0.2220\n",
      "Epoch 167/500\n",
      "547/547 [==============================] - 104s 191ms/step - loss: 4.6485 - acc: 0.2170 - val_loss: 4.5805 - val_acc: 0.2244\n",
      "Epoch 168/500\n",
      "216/547 [==========>...................] - ETA: 1:01 - loss: 4.6236 - acc: 0.2178"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "batch_size = 32\n",
    "history = autoencoder.fit_generator(data_generator(data), \n",
    "                                    steps_per_epoch=int((len(data)+batch_size-1)/batch_size), \n",
    "                                    validation_data=data_generator(val), \n",
    "                                    validation_steps=int((len(val)+batch_size-1)/batch_size),\n",
    "                                    epochs=1000, callbacks=[EarlyStopping(monitor='val_loss', patience=15), \n",
    "                                                           ModelCheckpoint(\"autoencoder.hdf5\", monitor='val_loss', save_best_only=True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.save('autoencoder.h5')\n",
    "encoder.save('encoder.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('history.pkl', 'wb') as f:\n",
    "    pickle.dump(history, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
